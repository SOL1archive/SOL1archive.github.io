3:I[9275,[],""]
5:I[1343,[],""]
6:I[3889,["231","static/chunks/231-c5de4feaddc0b512.js","185","static/chunks/app/layout-6c2c9e9b413bbfd5.js"],"default"]
7:I[1254,["231","static/chunks/231-c5de4feaddc0b512.js","185","static/chunks/app/layout-6c2c9e9b413bbfd5.js"],"default"]
4:["slug","Public/step2-5","c"]
0:["XyTXgP6qMgUjdp3ye3Nwy",[[["",{"children":[["slug","Public/step2-5","c"],{"children":["__PAGE__?{\"slug\":[\"Public\",\"step2-5\"]}",{}]}]},"$undefined","$undefined",true],["",{"children":[["slug","Public/step2-5","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"en","children":["$","body",null,{"children":["$","main",null,{"className":"min-h-screen bg-background text-foreground flex flex-col","children":[["$","$L6",null,{"gaId":""}],["$","$L7",null,{}],["$","div",null,{"className":"flex-1 w-full max-w-[1200px] mx-auto p-8","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":null}]}]]}]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/e7828a7281941fdd.css","precedence":"next","crossOrigin":"$undefined"}]],"$L8"]]]]
9:T3e27,<ul>
<li>TOC
{:toc}</li>
</ul>
<p>{:#튜닝 프로세스}</p>
<h2 id="튜닝-프로세스">튜닝 프로세스</h2>
<p>머신러닝 분야에서 좋은 하이퍼파라미터를 찾는 것은 중요한 문제다. 따라서 좋은 하이퍼파라미터를 체계적으로 구하는 것은 중요하다. 심층신경망을 학습시킬 때 다양한 하이퍼파라미터는 딥러닝 모델을 다루는 것을 어렵게 한다. 이전 포스트들에서 얼마나 많은 하이퍼파라미터가 존재하는지 알게되었을 것이다. 다음은 하이퍼파라미터들의 예시이다.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span></td>
<td>Learning Rate</td>
</tr>
<tr>
<td><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span></td>
<td>About Momentum Algorithm</td>
</tr>
<tr>
<td><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo separator="true">,</mo><msub><mi>β</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>β</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\epsilon, \beta _1, \beta _2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ϵ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></td>
<td>About Adam</td>
</tr>
<tr>
<td>#Layers</td>
<td>뉴런층의 개수</td>
</tr>
<tr>
<td>#Hidden Units</td>
<td>각 뉴런층의 뉴런의 개수</td>
</tr>
<tr>
<td>Learning Rate Decay</td>
<td>학습률 감쇠</td>
</tr>
<tr>
<td>mini-batch size</td>
<td>미니배치의 크기</td>
</tr>
</tbody>
</table>
<p>이 외에도 신경망 아키텍처에 따라서 하이퍼파라미터의 개수와 종류가 더 증가할 수 있다. 이 하이퍼파라미터들은 학습의 효율 뿐만 아니라 학습 후의 모델 성능에도 큰 영향을 미친다.</p>
<p>머신러닝의 초기에서는 모든 하이퍼파라미터의 경우를 전부 적용해보고 최적의 하이퍼파라미터를 찾는 것이 일반적이었다. 이를 격자 탐색(<em>Grid Search</em>)라고 한다. 하지만 하이퍼파라미터의 수가 매우 많아짐에 따라 그리드 서치는 비효율적이게 되었다. 대신 하이퍼파라미터의 경우의 수가 매우 많을 때는 무작위로 하이퍼파라미터들을 설정하고 각각에 대해 탐색하여 최적의 하이퍼파라미터 조합을 탐색한다.</p>
<p>다른 방식은 정밀화(<em>Coarse to fine</em>)이다. 만약 하이퍼파라미터 조합을 무작위로, 혹은 일정한 하이퍼파라미터 값의 단위에 따라 격자로 탐색을 해서 어떤 지점의 하이퍼파라미터가 가장 좋은 성능을 냈다고 하자. 정밀화를 적용할 때는 해당 지점의 하이퍼파라미터 부근에서 보다 세밀한 간격으로 하이퍼파라미터 탐색을 한다. 일반적으로 하이퍼파라미터 조합에 따른 성능은 연속(<em>Continuous</em>)일 것이므로 더 좋은 하이퍼파라미터 조합을 찾을 가능성이 크다. 이를 반복하여 충분히 좋은 성능의 하이퍼파라미터를 찾아낼 수 있다.</p>
<p>{:#적절한 척도(<em>Scale</em>) 선택하기}</p>
<h2 id="적절한-척도scale-선택하기">적절한 척도(<em>Scale</em>) 선택하기</h2>
<p>비록 하이퍼파라미터를 무작위로 탐색하는 것이 좋은 하이퍼파라미터를 찾는데 효율적이기는 하지만 '무작위'라는 것은 모든 가능한 값들 중 공평하게(<em>uniformly</em>) 값을 추출해야한다는 것은 아니다. 대신, 적절한 척도(<em>Scale</em>) 내에서 무작위로 값을 추출하는 것이 효율적이다. 가령 학습률 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span> 를 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0.0001</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.0001, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0.0001</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span> 내에서 공평하게 추출할 경우, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0.1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0.1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span> 을 탐색하는 데에는 대략 90%의 자원을, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0.0001</mn><mo separator="true">,</mo><mn>0.1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.0001, 0.1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0.0001</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0.1</span><span class="mclose">]</span></span></span></span></span> 을 탐색하는 데에는 10%의 자원을 사용하게 된다. 이는 효율적인 컴퓨팅 자원 분배가 아니다. 왜냐하면 실제로 학습률이 만들어내는 차이는 로그값의 크기에 비례할 것이기 때문이다. 값이 0에 가까울 수록 값이 만들어내는 차이가 더 커진다는 뜻이다. 따라서 로그스케일에 따라서 추출 비율을 정하는 것이 합리적일 것이다. 상용로그스케일을 적용할 경우 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0.0001</mn><mo separator="true">,</mo><mn>0.001</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.0001, 0.001]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0.0001</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0.001</span><span class="mclose">]</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0.001</mn><mo separator="true">,</mo><mn>0.01</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.001, 0.01]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0.001</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0.01</span><span class="mclose">]</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋯</mo></mrow><annotation encoding="application/x-tex">\cdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.313em;"></span><span class="minner">⋯</span></span></span></span></span> <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0.1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0.1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0.1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span> 에 대해 각각 일정한 비율로 추출할 것이다. 더 합리적인 추출 비율임을 확인할 수 있다. 이는 다음과 같이 파이썬 코드로도 구현가능하다.</p>
<pre><code class="hljs language-python">r = -<span class="hljs-number">4</span> * np.random.rand()
a = <span class="hljs-number">10</span> ** r
</code></pre>
<p>이처럼 적절한 척도를 설정하여 무작위로 하이퍼파라미터를 추출하는 것은 중요하다. 단순하게 선형적인 스케일에서 <em>uniform</em> 하게 추출하기보다 지수스케일, 로그스케일 등 적절한 척도를 적용하는 것이 더 합리적인 추출방법이다.</p>
<p>{:#하이퍼파라미터 튜닝 실전}</p>
<h2 id="하이퍼파라미터-튜닝-실전">하이퍼파라미터 튜닝 실전</h2>
<p>딥러닝이 적용되는 한 분야에 대한 하이퍼파라미터의 직관이 다른 분야에는 적용될 수도, 적용되지 않을 수도 있다. 가령 컴퓨터 비전에서 시작되었던 CNN(<em>Convolution Neural Network</em>)은 음성인식에도 적용이 되었고 더 나아가 자연어처리에도 적용이 된다. 이처럼 한 분야의 딥러닝 연구 결과는 다른 분야의 연구결과에 적용되기도 한다. 하지만 하이퍼파라미터 탐색은 한 분야에서의 직관이 다른 분야에서는 거의 적용되지 않는다. 또한 하이퍼파라미터는 AI가 작동하는 컴퓨터 환경, 데이터의 분포에 민감하게 반응하여 특정 하이퍼파라미터가 모든 경우에 적절하게 작동하는 경우는 거의 없다.</p>
<p>따라서 하이퍼파라미터를 찾을 때 크게 두가지 방법을 이용한다. 하나는 모델 돌보기이다. 이는 데이터는 충분하지만 CPU, GPU 등의 처리자원이 충분하지 않아 한번에 적은 수의 모델만 훈련할 수 있을 때 사용한다. 모델 돌보기에서는 하나의 학습 주기 내에서 일정 주기마다 학습 효율에 따라 하이퍼파라미터를 적절하게 조절한다. 일정 주기마다 모델의 학습 성능을 살펴보고 적절하게 조정한 다음, 조정된 하이퍼파라미터에 대한 학습 성능을 바탕으로 조정된 하이퍼파라미터를 사용할지, 이전의 하이퍼파라미터를 사용할지 결정한다. 해당 과정을 반복하면서 하이퍼파라미터 조정에 대한 피드백을 반복적으로 진행한다. 마치 모델을 매일매일 돌보면서 피드백을 얻기 때문에 모델 돌보기라고 한다. 모델 돌보기에서의 핵심은 매 피드백마다 학습된 파라미터를 재사용한다는 것이다. 학습된 파라미터를 계속 사용하기 때문에 장기적으로 매 피드백마다 지속적으로 비용함수는 감소한다.</p>
<p>다른 접근은 여러개의 모델을 병렬적으로 학습시키는 것이다. 하이퍼파라미터가 다른 여러 개의 모델을 생성하고 일정 기간 동안 계속 학습시킨다. 이 방법으로 여러 하이퍼파라미터에 대한 모델을 시도해보고 최고의 성능을 가진 모델을 선택하는 것이다.</p>
<p>학습하고자 하는 모델의 특성에 따라 적절한 접근 방식을 선택해야 한다. 단순한 모델의 경우 다수의 모델을 생성해서 병렬적으로 학습시키는 것이 용이하므로 병렬적으로 학습시키면서 하이퍼파라미터를 튜닝하는 방법이 적절하다고 할 수 있을 것이다. 반면 컴퓨터 비전이나 대규모 사전학습 모델의 경우 다수의 모델을 생성하는 것이 어려우므로 모델을 돌보면서 하이퍼파라미터를 튜닝하는 것이 적절할 것이다.</p>
<blockquote>
<p>본 노트는 Andrew Ng의 머신러닝 수업을 정리한 것임.
Andrew Ng, Machine learning lecture, <a href="https://www.youtube.com/playlist?list=PLkRLdi-c79HKEWoi4oryj-Cx-e47y_NcM">Youtube Link</a></p>
</blockquote>
<blockquote>
<p><a href="https://sol1archive.github.io/note/step2-4">이전 포스트</a> |  <a href="https://sol1archive.github.io/note/step2-6">다음 포스트</a></p>
</blockquote>2:["$","div",null,{"className":"page_layout__YXtDc","children":[["$","div",null,{"className":"GlassContainer_glass__BAl5w page_postContainer__8rQn1","style":"$undefined","children":["$","article",null,{"className":"$undefined","children":[["$","header",null,{"className":"page_header__LPYYk","children":[["$","h1",null,{"className":"page_title__Tljh5","children":"머신러닝 노트(2-5)"}],["$","div",null,{"className":"page_meta__FlrbA","children":[["$","time",null,{"className":"$undefined","children":"10-10-2022"}],"$undefined"]}]]}],["$","div",null,{"className":"page_content__kgYnh","dangerouslySetInnerHTML":{"__html":"$9"}}]]}]}],["$","aside",null,{"className":"page_sidebar__vVJp_","children":["$","nav",null,{"className":"TOC_toc__LZ8ns","children":[["$","h4",null,{"className":"TOC_title__k1O4v","children":"On This Page"}],["$","ul",null,{"className":"TOC_list__jQuW3","children":[["$","li","0",{"className":"TOC_item__f3x9l TOC_level-2__lCxnK","children":["$","a",null,{"href":"#","children":"튜닝 프로세스"}]}],["$","li","1",{"className":"TOC_item__f3x9l TOC_level-2__lCxnK","children":["$","a",null,{"href":"#scale","children":"적절한 척도(_Scale_) 선택하기"}]}],["$","li","2",{"className":"TOC_item__f3x9l TOC_level-2__lCxnK","children":["$","a",null,{"href":"#","children":"하이퍼파라미터 튜닝 실전"}]}]]}]]}]}]]}]
8:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"SOL1 Archive"}],["$","meta","3",{"name":"description","content":"Personal Research Blog"}],["$","link","4",{"rel":"icon","href":"/favicon.ico"}]]
1:null
