# 머신러닝의 수학적 구현
## 뉴런의 모델링
머신러닝은 기본적으로 신경망의 작동구조를 모방한다. 따라서 머신러닝을 컴퓨터로 구현하기 위해서는 신경망의 작동원리를 수학적으로 모델링하는 것이 필수적이다. 신경망의 기본단위는 뉴런이므로, 신경망을 모델링하기 위해선 우선 뉴런을 모델링해야 한다. 뉴런은 다음과 같이 모델링할 수 있다. $w_i$를 가중치(_weight_), $b$를 편향(_bias_)라고 한다.
$$
y = f_{activ}(w_1x_1 + w_2x_2 + w_3x_3 + \cdots + w_nx_n + b) \\
= f_{activ}(\sum_{k=1}^nw_k x_k + b)
$$
이렇게 모델링된 뉴런을 퍼셉트론이라고 부른다. 활성화 함수인 $f_{activ}$는 일반적으로 두 식 중 하나를 사용한다. 첫째 식은 단위계단함수로, 실제 뉴런의 활성화와 유사하지만, $x=0$ 에서 미분 불가하다는 특징을 가지고 있다. 둘째 식은 시그모이드함수로, 정의역 $\mathbb R$에 대해 미분이 가능하다.
$$
f_{activ}(x) = 
\begin{cases}
1,\; x > 0 \\
0,\; x < 0
\end{cases} \\
f_{activ}(x) = 
\frac{1}{1 + e^{-x}}
$$
딥러닝은 이와 같이 모델링된 뉴런들을 다중 레이어에 배치하여 구현한다.
### 벡터
뉴런의 수학적 모델링은 이와 같이 표현가능하지만, 더 간편한 표현을 위해 벡터와 행렬을 사용한다. 벡터는 기저의 실수배의 합을 통해 정의된다. 기저는 벡터공간을 선형생성하는, 선형독립인 단위벡터로 정의한다. 쉽게 표현하면, 임의의 $N$차원 공간의 $x$축, $y$축과 같은 축의 양의 방향을 가지는, 크기가 1인 벡터이다. 기저는 $\overrightarrow{e_1},\;\overrightarrow{e_2},\;\cdots,\;\overrightarrow{e_n}$과 같이 표현한다. 벡터는 기저들의 실수배의 합으로 정의된다. 가령 벡터$\overrightarrow{v}$의 시점이 $(0, 0)$이고, 종점이 $(3, 2)$일 때, 벡터 $\overrightarrow{v}$는 다음과 같이 표현가능하다.
$$
\overrightarrow{v} = 3\overrightarrow{e_1} + 2\overrightarrow{e_2}=\;(3, 2)
$$

벡터의 연산 중에는 스칼라곱(_dot product_)과 벡터곱(_cross product_)가 있다. 그 중 스칼라곱은 다음과 같이 정의할 수 있다.
$$
\overrightarrow{a} := (a_1,\;a_2,\;\cdots,\;a_n),\;\overrightarrow{b} := (b_1,\;b_2\;\cdots,\;b_n) \\
\overrightarrow{a} \cdot \overrightarrow{b} = a_1b_1 + a_2b_2 + \cdots + a_nb_n
= \sum_{k=1}^na_kb_k
$$
스칼라곱을 이용하면 뉴런의 모델링을 다음과 같이 더 간략하게 표현할 수 있다. 또, 벡터들을 다시 행렬로 정의하여 행렬곱을 사용하면 신경망을 더 간략하게 기술할 수 있다.
$$
\overrightarrow{w} := (w_1, w_2, \cdots, w_n),\;\overrightarrow{x} := (x_1, x_2, \cdots, x_n) \\
y = f_{activ}(\overrightarrow{w} \cdot \overrightarrow{x} + b)
$$

## 회귀분석
회귀분석은 변수 혹은 변수들(_feature_)과 다른 한 변수(_target_)의 관계를 분석하는 통계적 기법이다. 회귀분석은 머신러닝에서 광범위하게 쓰이지만, 그 중 선형회귀는 신경망 학습에 쓰이기도 한다. 선형회귀는 직선의 방정식을 회귀방정식으로 사용하는 회귀분석이다. 가령 다음과 같은 데이터가 있다고 해보자.
|$x$|$y$|
|-|-|
|$x_1$|$y_1$|
|$x_2$|$y_2$|
|$\vdots$|$\vdots$|
|$x_n$|$y_n$|

이 데이터에 타겟을 $y$, 피처를 $x$로하는 선형회귀를 한다는 것은, 직선의 방정식 $y = px + q$를 이용해 두 데이터 간의 관계를 표현하는 것이다. 이 때 각 데이터와 실제의 오차 $e_k$는 $e_k = y_k - (px_k + q)$과 같을 것이다. 그런데 이 오차의 부호는 양수, 음수 모두 될 수 있으므로, 다루기에는 적합하지 않다. 따라서 오차를 다룰 때에는 오차값에 제곱을 하여 ${e_k}^2$으로 다룬다. 회귀 분석의 오차함수는 다움과 같이 나타낼 수 있다.
$$
C_T = \sum_{k=1}^n{e_k}^2 = \sum_{k=1}^n(y_k - (px_k + q))^2
$$
오차함수의 정의역이 ${\mathbb R}^N$이고, 정의역 전체 원소에 대해 미분가능하다고 하자. 그렇다면 페르마의 정리를 $N$차원으로 확장하여(최솟값의 필요조건), 오차함수가 최솟값을 가지는 지점은 다음 식을 만족시킬 것이라 추론할 수 있다.
$$
\dfrac{\partial\;C_T}{\partial p} = 0\;\land\;\dfrac{\partial\;C_T}{\partial q} = 0
$$

### 경사하강법
