---
title: 머신러닝 노트(2-6)
feed: show
mathjax: true
toc: true
---

* TOC
{:toc}

{:#배치 정규화(_Batch Normalization_)}
## 배치 정규화(_Batch Normalization_)
배치 정규화 알고리즘은 하이퍼파라미터 탐색을 더 쉽게 만들어줄 뿐만 아니라 데이터와 파라미터 간의 상관관계를 줄여준다. 또한 매우 깊은 신경망도 쉽게 학습할 수 있게 해준다. 이전에 입력변수를 정규화해주면 학습의 속도가 빨라졌다. 다음과 같은 정규화 식을 적용하면 비용함수의 Level Curve가 경사하강법에 더 적합한 둥근 형태로 바뀌기 때문에 가능한 것이다.

$$
\begin{aligned}
\mu &= \frac{1}{m} \sum_i x^{(i)}\\
X :&= X - \mu\\
\sigma ^2 &= \frac{1}{m} \sum_i (x^{(i)})^2\\
X :&= \frac{X}{\sigma}
\end{aligned}
$$

이 아이디어를 심층신경망에 어떻게 적용해줄 수 있을까? 우리는 이전에 은닉층이 이전 신경망 층의 출력을 피처(입력)으로 갖는 신경망임을 확인했다. 따라서 이전층의 출력을 정규화한 후에 다음 은닉층에 대입하면 앞서 설명한 정규화의 효과를 모든 신경망에서 얻을 수 있을 것이다. 이제 중요한 문제는 은닉층의 학습을 빠르게 하기 위해 이전 층의 출력을 어떻게 정규화할 것인지 이다. 이에 대한 해결책이 배치 정규화이다. 한번 심층신경망의 일부를 살펴보자.

$$
\begin{aligned}
a^{[2]} &= g^{[2]} \left(z^{[2]} \right)\\
z^{[3]} &= W^{[3]\ T}\ a^{[2]} + b^{[3]}\\
\end{aligned}
$$

배치 정규화에서는 $z^{[2]}$ 를 정규화한다. 비록 $a^{[2]}$ 와 $z^{[2]}$ 중 무엇을 정규화하는 것이 좋은 지에 대해 논쟁이 있기는 하지만 $z^{[2]}$ 를 정규화하는 것이 더 많이 쓰인다. 

이제 어떻게 배치정규화를 구현하는지 살펴보자. 한 층의 $z^{[l]}$ 의 원소 $z^{(1)}, z^{(2)}, \cdots, z^{(m)}$ 에 평균( $\mu$ )과 분산( $\sigma ^2$ ) 를 다음과 같이 구한다.

$$
\begin{aligned}
\mu &= \frac{1}{m} \sum^m_{i = 1} z^{(i)} \\
\sigma ^2 &= \frac{1}{m} \sum^m_{i = 1} (z^{(i)} - \mu)^2 \\
z^{(i)}_{norm} &= \frac{z^{(i)} - \mu}{\sqrt{\sigma ^2 + \epsilon}} \\
\end{aligned}
$$

세번째 식의 분모에 $\epsilon$ 을 더하는 이유는 수치안정성을 획득하기 위해서이다. 이와 같은 표준화를 수행하면 평균이 0, 분산이 1인 분포를 가지는 피처를 얻게된다. 하지만 출력값은 다양한 것이 좋으므로 다음과 같이 ${\tilde z}^{(i)}$ 를 다음과 같이 구한다.

$$
{\tilde z}^{(i)} = \gamma z^{(i)}_{norm} + \beta
$$

이 때 $\gamma, \beta$ 는 학습시킬 수 있는 변수이다. 따라서 다양한 최적화 알고리즘을 통해 두 변수를 갱신할 수 있다. $\gamma, \beta$ 에 따라 값을 배치 정규화 이전으로 되돌리는 것도 가능하다. 만약 두 값이 다음과 같다고 하자.

$$
\begin{array}{ll}
Let, \\
&\begin{aligned}
\gamma &= \sqrt{\sigma ^2 + \epsilon} \\
\beta &= \mu
\end{aligned} \\
Then, \\
& {\tilde z}^{(i)} = z^{(i)}
\end{array}
$$

이처럼 $\gamma, \beta$ 를 이용하면 $z^{(i)}$ 값에 대한 적절한 스케일링이 가능해진다. 이렇게 얻은 ${\tilde z}^{(i)}$ 활성화 함수에 대입하여 출력값을 구한다.

---
title: 머신러닝 노트(2-7,8)
feed: show
mathjax: true
toc: true
---

* TOC
{:toc}

{:#배치 정규화의 적용}
## 배치 정규화의 적용
이제 심층신경망에서 어떻게 배치 정규화를 적용할지 알아보자. 만약 배치 정규화를 적용하지 않으면 2개의 은닉층을 가진 신경망은 다음과 같이 표현될 것이다.

$$
\begin{aligned}
z^{[1]} &= W^{[1]\ T}\ X + b^{[1]}\\
a^{[1]} &= g^{[1]} \left(z^{[1]} \right)\\
z^{[2]} &= W^{[2]\ T}\ a^{[1]} + b^{[2]}\\
a^{[2]} &= g^{[2]} \left(z^{[2]} \right)\\
z^{[3]} &= W^{[3]\ T}\ a^{[2]} + b^{[3]}\\
a^{[3]} &= g^{[3]} \left(z^{[3]} \right)\\
\end{aligned}
$$

배치 정규화는 이 식들에서 활성화 함수에 대입하기 전에 각각 적용된다. 따라서 식은 다음과 같다.

$$
\begin{array}{ll}
\begin{aligned}
    & \phantom{aaaa} z^{[1]} = W^{[1]\ T}\ X + b^{[1]}\\
    \text{BN}: & \ 
    \left(
    \begin{aligned}
        z^{[1]}_{norm} &= \frac{z^{[1]} - \mu^{[1]}}{\sqrt{(\sigma ^{[1]})^2 + \epsilon}}\\
        {\tilde z}^{[1]} &= \gamma ^{[1]} z^{[1]}_{norm} + \beta ^{[1]}\\
    \end{aligned} \right. \\
    & \phantom{aaaa} a^{[1]} = g^{[1]} \left({\tilde z}^{[1]} \right)\\
    \ \\
    & \phantom{aaaa} z^{[2]} = W^{[2]\ T}\ a^{[1]} + b^{[2]}\\
    \text{BN}:&\ 
    \left(
    \begin{aligned}
        z^{[2]}_{norm} &= \frac{z^{[2]} - \mu^{[2]}}{\sqrt{(\sigma ^{[2]})^2 + \epsilon}}\\
        {\tilde z}^{[2]} &= \gamma ^{[2]} z^{[2]}_{norm} + \beta ^{[2]}\\
    \end{aligned} \right. \\
    & \phantom{aaaa} a^{[2]} = g^{[2]} \left({\tilde z}^{[2]} \right)\\
    \ \\
    & \phantom{aaaa} z^{[3]} = W^{[3]\ T}\ a^{[2]} + b^{[3]}\\
    \text{BN}:&\ 
    \left(
    \begin{aligned}
        z^{[2]}_{norm} &= \frac{z^{[2]} - \mu^{[1]}}{\sqrt{(\sigma ^{[2]})^2 + \epsilon}}\\
        {\tilde z}^{[2]} &= \gamma ^{[2]} z^{[2]}_{norm} + \beta ^{[2]}\\
    \end{aligned} \right. \\
    & \phantom{aaaa} a^{[3]} = g^{[3]} \left({\tilde z}^{[3]} \right)\\
\end{aligned}
\ \\
\ \\
\text{(BN: Batch Normalization)}\\
\end{array}
$$

따라서 파라미터들은 다음과 같다.
$$
\begin{array}{ll}
Parameters: &W^{[1]},\ b^{[1]},\ W^{[2]},\ b^{[2]},\ \cdots,\ W^{[l]},\ b^{[l]} \\
            &\gamma ^{[1]},\ \beta ^{[1]},\ \gamma ^{[2]},\ \beta ^{[2]},\ \cdots, \gamma ^{[l]},\ \beta ^{[l]}\\
\end{array}
$$

유의해야 할 것은 모멘텀이나 Adam 알고리즘에 사용되는 $\beta$ 와 위의 $\beta ^{[l]}$ 가 다르다는 것이다. 위의 $\beta ^{[l]}$ 은 학습으로 조정되는 파라미터인 반면, 최적화 알고리즘에 사용되는 $\beta$ 는 하이퍼파라미터이다.

미니배치를 이용해 학습할 때, 배치 정규화에 사용할 평균과 분산을 구할 때는 각 미니배치에 대한 연산 결과 값의 평균과 분산으로만 구해야 한다. 동시에 $\beta, \gamma$ 를 구할 때도 하나의 미니배치에 대한 값을 구해야 한다. 그러므로 자연스럽게 역전파를 통해 파라미터를 갱신할 때도 각 미니배치에 대한 비용함수를 이용해 값을 갱신해야 한다.

또 확인할 수 있는 것은, 배치 정규화를 적용하면 편향은 사실상 없다고 봐도 무방하다는 것이다. 이는 다음과 같이 증명 가능하다.

임의의 한 층 $l$ 에 대한 연산은 다음과 같다.

$$
\begin{aligned}
    & \phantom{aaaa} z^{[l]} = W^{[l]\ T}\ X + b^{[l]}\\
    \text{BN}: & \ 
    \left(
    \begin{aligned}
        z^{[l]}_{norm} &= \frac{z^{[l]} - \mu^{[l]}}{\sqrt{(\sigma^{[l]})^2 + \epsilon}}\\
        {\tilde z}^{[l]} &= \gamma ^{[l]} z^{[l]}_{norm} + \beta ^{[l]}\\
    \end{aligned} \right. \\
    & \phantom{aaaa} a^{[l]} = g^{[l]} \left({\tilde z}^{[l]} \right)\\
\end{aligned}
$$

첫번째 식부터 세번째식 까지 순차적으로 대입해보자.

$$
\begin{array}{ll}
\begin{aligned}
    z^{[l]} &= W^{[l]\ T}\ X + b^{[l]}\\
    z^{[l]}_{norm} &= \frac{W^{[l]\ T}\ X + b^{[l]} - \mu^{[l]}}{\sqrt{(\sigma^{[l]})^2 + \epsilon}} = \frac{W^{[l]\ T}\ X}{\sqrt{(\sigma^{[l]})^2 + \epsilon}} + \frac{b^{[l]} - \mu^{[l]}}{\sqrt{(\sigma^{[l]})^2 + \epsilon}}\\
    {\tilde z}^{[l]} &= \gamma ^{[l]} \frac{W^{[l]\ T}\ X}{\sqrt{(\sigma^{[l]})^2 + \epsilon}} + \gamma ^{[l]} \frac{b^{[l]} - \mu^{[l]}}{\sqrt{(\sigma^{[l]})^2 + \epsilon}} + \beta ^{[l]}\\
\end{aligned}\\
&\blacksquare
\end{array}
$$

세번째 식을 자세히 살펴보면 $\gamma ^{[l]} \frac{b^{[l]} - \mu^{[l]}}{\sqrt{(\sigma^{[l]})^2 + \epsilon}} + \beta ^{[l]}$ 이 하나의 상수로 묶임을 확인할 수 있다. 따라서 딥러닝의 연산을 다음과 같다고 보고 연산할 수 있는 것이다.

$$
\begin{aligned}
    z^{[l]} &= W^{[l]\ T}\ X\\
    z^{[l]}_{norm} &= \frac{z^{[l]} - \mu^{[l]}}{\sqrt{(\sigma^{[l]})^2 + \epsilon}}\\
    {\tilde z}^{[l]} &= \gamma ^{[l]} z^{[l]}_{norm} + \beta ^{[l]}\\
     \\
    a^{[l]} &= g^{[l]} \left({\tilde z}^{[l]} \right)\\
\end{aligned}
$$

이는 배치 정규화 중 표준화 과정이 **평균을 0 으로 만들기 때문에 편향이 필요 없는 것** 이라고도 이해할 수 있다. 따라서 배치 정규화를 적용했을 때는 편향이 없거나 $\mathbb{0}$ 이라고 보고 연산을 수행할 수 있다.

---
title: 머신러닝 노트(2-7,8)
feed: show
mathjax: true
toc: true
---

* TOC
{:toc}

{:#배치 정규화가 잘 작동하는 이유}
## 배치 정규화가 잘 작동하는 이유
피처에 대한 정규화는 일반적으로 퍼셉트론의 학습 성능을 높여준다. 앞서 서술했듯이 신경망의 각 층은 이전 층의 출력을 피처로 하여 연산을 수행한다. 따라서 각 층의 활성화 값에 정규화를 적용하면 다음 층의 학습 성능이 높아질 것이고, 결과적으로 전체 모델의 학습 성능이 높아질 것이다.

다른 이유는 공변량 변화(_Covariate Shift_)와 관련이 깊다. 공변량 변화는 데이터의 분포가 바뀔 때(= 다른 형태의 데이터셋으로 교체되었을 때) 모델의 성능이 유지되는 것을 기대하기 어려운 것을 의미한다. 가령 모델을 훈련시킬 때 실제 고양이 사진을 훈련 데이터로 사용한 후에 모델을 고양이 그림 데이터를 분류하는데에 사용하면 그리 좋은 성능을 기대하지 못할 것이다. 모델의 입력 피처와 예측 목표가 동일해도 데이터의 분포가 달라지면 모델의 성능이 낮아지는 것이 바로 공변량 변화이다.

딥러닝 모델의 학습중에는 반복적으로 공변량 변화가 일어난다. 학습중에는 특정 은닉층에 대해 이전 층의 파라미터 값들이 계속 갱신된다. 이전 층의 파라미터 값이 갱신되므로 출력값의 분포 또한 변하게 된다. 따라서 딥러닝 학습에서 이전 층의 파라미터 갱신이 공변량 변화를 일으킨다.

배치 정규화는 이러한 공변량 변화를 억제할 수 있다. 이전 층의 파라미터 갱신은 출력값의 분포의 변화를 의미한다. 그런데 배치 정규화는 출력값의 분포가 어떻게 변하든지 일정한 평균과 표준편차를 가지도록 보정하므로 다음층에 전달되는 출력값의 분포는 일정하게 된다. 결과적으로 배치 정규화는 이전 층의 파라미터가 어떻게 변하든 다음 층의 파라미터들이 가지는 그에 맞추어서 변화되어야 하는 부담을 줄여준다. 이는 전체 신경망의 학습 성능을 높이는 결과로 이어진다.

배치 정규화의 다른 효과는 규제 효과(_Regularization Effect_)이다. 미니 배치는 전체 데이터셋에 대해 무작위 추출을 한 것이라고 볼 수 있으므로, 각 미니 배치의 평균과 표준편차는 전체 데이터셋에 대한 평균과 표준편차와 차이가 있을 것이다. 따라서 각 미니 배치에 대한 배치 정규화에서의 평균과 표준편차 또한 전체 데이터에 대한 평균과 표준편차와 다를 것이다. 이는 각 층의 출력에 곱셈 잡음과 덧셈 잡음을 더하는 효과를 낼 것이다. 

곱셈잡음이란 무작위의 값을 입력값에 곱하는 것을 의미하고, 덧셈잡음이란 무작위의 값을 입력값에 더하는 것을 의미한다. 드롭아웃은 0 혹은 1을 한 층의 출력값에 곱하여 다음 층의 피처로 전달하므로 곱셈잡음을 적용하는 Regularization이라고 볼 수 있을 것이다. 한편 배치 정규화는 어느정도 잡음이 섞인(= 실제와 차이가 있는) 평균과 표준편차에 대해 평균을 빼고 분산을 곱하므로, 미니배치에 대해서는 곱셈잡음과 덧셈잡음 모두를 내는 Regularization이라고 볼 수 있다. 하지만 미니배치의 규모가 더 클수록 평균과 표준편차 모두 실제 평균, 표준편차에 수렴할 것이므로(큰 수의 법칙), 미니배치의 규모가 클수록 Regularization의 효과는 감소할 것이다. 그러므로 Regularization의 목적으로 배치 정규화를 사용하기보다는 부가적인 효과정도로 생각하는 것이 바람직할 것이다.

---
title: 머신러닝 노트(2-7,8)
feed: show
mathjax: true
toc: true
---

* TOC
{:toc}

{:#테스트 시의 배치 정규화}
## 테스트 시의 배치 정규화
배치 정규화는 한번에 하나의 미니배치를 처리하면서 학습한다. 하지만 테스트의 경우에는 한번에 하나의 데이터에 대해 처리한다. 따라서 테스트 시의 배치 정규화에 대한 접근은 학습시와 약간 다르다.

학습시 배치 정규화를 수행할 때는 미니배치에 대한 평균과 분산을 구했다. 하지만 테스트 과정에서는 미니배치를 이용한 평균과 분산을 구할 수 없으므로 다른 방식으로 처리해야 한다. 따라서 테스트 시기에는 미리 구한 평균과 분산을 적용한다.

여기서 평균과 분산을 구하는 대표적인 방법은 여러 미니배치에 걸쳐서 구한 각 평균과 분산에 대한 지수가중 이동평균을 구하는 것이다. 왜 지수가중 이동평균일까? 학습을 통해 파라미터가 갱신되면서 출력값의 분포가 변하고 결과적으로 평균과 표준편차 모두가 변화한다. 그리고 데이터에 대한 모델의 비용함수 값은 학습이 진행됨에 따라 낮아지는 경향을 가진다. 따라서 미니배치의 평균과 분산을 구할 때 단순한 산술평균이나 기하평균을 적용하는 것은 불합리하므로, 지수가중 이동평균을 적용해서 평균과 분산을 구하는 것이다.

> 본 노트는 Andrew Ng의 머신러닝 수업을 정리한 것임. 
> Andrew Ng, Machine learning lecture, [Youtube Link](https://www.youtube.com/playlist?list=PLkRLdi-c79HKEWoi4oryj-Cx-e47y_NcM)

> [이전 포스트](https://sol1archive.github.io/note/step2-5) |  [다음 포스트](https://sol1archive.github.io/note/step2-7,8)
